{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment analysis\n",
    "\n",
    "For this project, we will compute the <b>semantic orientation</b> score which tells us whether a term (therefore a tweet) is more closely related to a positive or to a negative vocabulary. It's particulary interesting to notice that this approach is <b>non supervised</b>, that is, requires no labeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 3)\n",
      "56.46 percents of tweets are positives\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
    "print(data.shape)\n",
    "print('%.2f percents of tweets are positives'%(data.Sentiment.sum()*100/len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that tweets need a cleaning step:  \n",
    "- Removing white spaces and setting lowercase as default\n",
    "- Removing url\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tweets processed\n",
      "10000 tweets processed\n",
      "20000 tweets processed\n",
      "30000 tweets processed\n",
      "40000 tweets processed\n",
      "50000 tweets processed\n",
      "60000 tweets processed\n",
      "70000 tweets processed\n",
      "80000 tweets processed\n",
      "90000 tweets processed\n",
      "CPU times: user 3min 29s, sys: 31.6 s, total: 4min 1s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SentimentText = []\n",
    "for i,text in enumerate(data.SentimentText):\n",
    "    if i%10000 == 0:\n",
    "        print(i,'tweets processed')\n",
    "    \n",
    "    # Remove white spaces before and after, and set lowercase\n",
    "    t = text.strip().lower()\n",
    "    \n",
    "    # Remove url\n",
    "    t = re.sub(r'http(s)?:?//[\\w$.…@&+-/]*',' ',t)\n",
    "    t = re.sub(r'(www\\.)(\\w|\\.)*', ' ', t)\n",
    "    \n",
    "    # Remove mutliple letters (ex : I looove it !)\n",
    "    for i in range(25,1,-1):\n",
    "        for voyelle in ['a','e','i','o','u','y',' ','.']:\n",
    "            t = t.replace(voyelle*i, voyelle)\n",
    "    for i in range(25,2,-1):\n",
    "        for consonne in ['b','c','d','f','g','h','j','k','l','m','n','p','q','r','s','t','v','w','x','z']:\n",
    "            t = t.replace(consonne*i, consonne)\n",
    "\n",
    "    # Remove @mention, hashtag and esperluettes\n",
    "    t = re.sub(r'(@|#|&)(\\w)*',' ',t)\n",
    "    \n",
    "    # Remove remaining non-text characters\n",
    "    t = re.sub(r'[^\\w\\s]',' ',t) # everything\n",
    "    t = re.sub(r'[\\d]+',' ',t)   # numbers\n",
    "    t = re.sub(r'[_]+',' ',t)   # underscore '_' \n",
    "    \n",
    "    # Remove white spaces IN the tweets\n",
    "    t = re.sub(r'[\\s]{2,}',' ',t).strip()\n",
    "    t = unicodedata.normalize('NFD', t).encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    # We consider words formed by less than 3 letters are non-informative\n",
    "    t = [mot for mot in t.split() if len(mot)>=3]\n",
    "    # Remove stop-words\n",
    "    t = [mot for mot in t if mot not in stopwords.words('english')]\n",
    "    t = ' '.join(t)\n",
    "    \n",
    "    SentimentText.append(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>missed new mon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga gunna cry ben dentist since suposed get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>think cheating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                                     sad apl friend\n",
       "1       2          0                             missed new mon trailer\n",
       "2       3          1                                        omg already\n",
       "3       4          0  omgaga gunna cry ben dentist since suposed get...\n",
       "4       5          0                                     think cheating"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SentimentText'] = pd.Series(SentimentText)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('dataset_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic orientation\n",
    "\n",
    "For some positively connotated vocabulary $V^{+}$ and some negatively connotated vocabulary $V^{-}$, the **semantic orientation** (SO) of a term $t$ is defined as follows :\n",
    "$$SO(t) = \\sum_{t' \\in V^{+}} PMI(t,t') - \\sum_{t' \\in V^{-}} PMI(t,t'),$$\n",
    "where $PMI(t_1, t_2)$ is a proximity measure (https://en.wikipedia.org/wiki/Pointwise_mutual_information) given as \n",
    "$$PMI(t_1,t_2) = \\ln\\Large(\\normalsize\\frac{\\mathbb{P}(t_1, t_2)}{\\mathbb{P}(t_1).\\mathbb{P}(t_2)}\\Large)\\normalsize = \\ln\\Large(\\normalsize \\frac{ \\frac{DF(t_1,t_2)}{D} }{\\frac{DF(t_1)}{D}.\\frac{DF(t_2)}{D}}\\Large)\\normalsize$$\n",
    "\n",
    "In other words, it defines how close a term $t$ is to a positive vocabulary, composed of $D$ documents (tweets). $DF(t,t')$ refers to the document frequency, i.e. the numbers of documents in which both $t$ and $t'$ occur. \n",
    "\n",
    "So eventually, we need to compute for each term $t$ in our corpus the following :\n",
    "$$SO(t) = \\sum_{t' \\in V^{+}} \\ln\\Large(\\normalsize \\frac{ \\frac{DF(t,t')}{D} }{\\frac{DF(t)}{D}.\\frac{DF(t')}{D}}\\Large)\\normalsize ~ - \\sum_{t' \\in V^{-}} \\ln\\Large(\\normalsize \\frac{ \\frac{DF(t,t')}{D} }{\\frac{DF(t)}{D}.\\frac{DF(t')}{D}}\\Large)\\normalsize ,$$\n",
    "\n",
    "which simplifies as \n",
    "\n",
    "$$\\text{$\\LARGE ($}\\sum_{t' \\in V^{+}} \\ln\\text{$\\large ($}DF(t,t')\\text{$\\large )$} - \\ln\\text{$\\large ($}DF(t)\\text{$\\large )$} - \\ln\\text{$\\large ($}DF(t')\\text{$\\large )$} + \\ln(D) \\text{$\\LARGE )$ } - \\text{$\\LARGE ($}\\sum_{t' \\in V^{-}} \\ln\\text{$\\large ($}DF(t,t')\\text{$\\large )$} - \\ln\\text{$\\large ($}DF(t)\\text{$\\large )$} - \\ln\\text{$\\large ($}DF(t')\\text{$\\large )$} + \\ln(D) \\text{$\\LARGE )$ }$$ \n",
    "\n",
    "For a (relatively) short corpus like the one we 've got, $DF(t,t')$ and $DF(t')$ may be equals to zero, leading respectively $\\ln(DF(t,t'))$ and $\\ln(DF(t'))$ to be undefined. For this reason, we will assume log(0) = 0 as stated p. 12 here  : https://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get $V^{+}$ and $V^{-}$ from the existing lexicons made by Bing Liu : http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V_plus  = pd.read_table('opinion-lexicon-English/positive-words.txt', encoding='latin1')[33:].values\n",
    "V_minus = pd.read_table('opinion-lexicon-English/negative-words.txt', encoding='latin1')[33:].values\n",
    "V_plus  = np.ravel(V_plus).tolist()\n",
    "V_minus = np.ravel(V_minus).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(V_plus))\n",
    "V_plus[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(V_minus))\n",
    "V_minus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79991, 79991, 19998, 19998)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid , y_train, y_valid = train_test_split(data.SentimentText, \n",
    "                                                       data.Sentiment.values,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=123, shuffle=True,\n",
    "                                                       stratify=data.Sentiment.values)\n",
    "for df in ['x_train', 'x_valid']:\n",
    "    eval(df).reset_index(drop=True, inplace=True)\n",
    "    \n",
    "len(x_train), len(y_train), len(x_valid), len(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents-terms and co-occurence matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents-terms matrix\n",
    "This matrix will be used to efficiently compute $DF(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 39073)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "docs_terms = cv.fit_transform(x_train)\n",
    "print(docs_terms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have <b>79991</b> documents, resulting in a <b>39073</b> words vocabulary (for x_train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurence matrix\n",
    "This matrix will be used to efficiently compute $DF(t,t')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 39073)\n",
      "CPU times: user 1.97 s, sys: 2.62 s, total: 4.6 s\n",
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "co_occurence = docs_terms.transpose().dot(docs_terms)\n",
    "docs_terms   = pd.DataFrame(docs_terms.toarray(), columns=cv.get_feature_names()) \n",
    "co_occurence = pd.DataFrame(co_occurence.toarray(), columns=cv.get_feature_names(), index=cv.get_feature_names()) \n",
    "print(co_occurence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79991, 39073)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaww</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abagail</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abb</th>\n",
       "      <th>abba</th>\n",
       "      <th>...</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zune</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zuri</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zushi</th>\n",
       "      <th>zwart</th>\n",
       "      <th>zwijger</th>\n",
       "      <th>zwinky</th>\n",
       "      <th>zxcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaww  aba  aback  abagail  abandon  abandoned  abandoning  abangan  abb  \\\n",
       "0     0    0      0        0        0          0           0        0    0   \n",
       "1     0    0      0        0        0          0           0        0    0   \n",
       "2     0    0      0        0        0          0           0        0    0   \n",
       "3     0    0      0        0        0          0           0        0    0   \n",
       "4     0    0      0        0        0          0           0        0    0   \n",
       "\n",
       "   abba  ...   zumba  zune  zurg  zuri  zurich  zushi  zwart  zwijger  zwinky  \\\n",
       "0     0  ...       0     0     0     0       0      0      0        0       0   \n",
       "1     0  ...       0     0     0     0       0      0      0        0       0   \n",
       "2     0  ...       0     0     0     0       0      0      0        0       0   \n",
       "3     0  ...       0     0     0     0       0      0      0        0       0   \n",
       "4     0  ...       0     0     0     0       0      0      0        0       0   \n",
       "\n",
       "   zxcv  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 39073 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs_terms.shape)\n",
    "docs_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 39073)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaww</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abagail</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abb</th>\n",
       "      <th>abba</th>\n",
       "      <th>...</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zune</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zuri</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zushi</th>\n",
       "      <th>zwart</th>\n",
       "      <th>zwijger</th>\n",
       "      <th>zwinky</th>\n",
       "      <th>zxcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaww</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aba</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abagail</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaww  aba  aback  abagail  abandon  abandoned  abandoning  abangan  \\\n",
       "aaww        1    0      0        0        0          0           0        0   \n",
       "aba         0    2      0        0        0          0           0        0   \n",
       "aback       0    0      1        0        0          0           0        0   \n",
       "abagail     0    0      0        1        0          0           0        0   \n",
       "abandon     0    0      0        0        4          0           0        0   \n",
       "\n",
       "         abb  abba  ...   zumba  zune  zurg  zuri  zurich  zushi  zwart  \\\n",
       "aaww       0     0  ...       0     0     0     0       0      0      0   \n",
       "aba        0     0  ...       0     0     0     0       0      0      0   \n",
       "aback      0     0  ...       0     0     0     0       0      0      0   \n",
       "abagail    0     0  ...       0     0     0     0       0      0      0   \n",
       "abandon    0     0  ...       0     0     0     0       0      0      0   \n",
       "\n",
       "         zwijger  zwinky  zxcv  \n",
       "aaww           0       0     0  \n",
       "aba            0       0     0  \n",
       "aback          0       0     0  \n",
       "abagail        0       0     0  \n",
       "abandon        0       0     0  \n",
       "\n",
       "[5 rows x 39073 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(co_occurence.shape)\n",
    "co_occurence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example one can spot DF('suck','sucks'), that is, the tweets featuring both \"suck\" and \"sucks\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "20402 life sucks would suck lot beautiful check yes gingerette kep company\n",
      "72735 well sucks people suck havent met one honestly nice person today evil\n"
     ]
    }
   ],
   "source": [
    "print(co_occurence.loc['sucks','suck'])\n",
    "idx = docs_terms[(docs_terms['sucks'] != 0) & (docs_terms['suck'] != 0)].index\n",
    "for i in idx:\n",
    "    print(i, x_train.ix[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF(t), DF(t,t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find an efficient way to compute DF(t), that is, the number of documents when a term $t$ appears. Beware it's not the same as \"the number of times when t occurs in the documents\" ! In this second case, we would simply read DF(t) as `co_occurence.loc[t,t]`. So the diagonal of `co_occurence` gives us <b>TF(t)</b>, while we are looking for <b>DF(t)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaww</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abagail</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abangan</th>\n",
       "      <th>abb</th>\n",
       "      <th>abba</th>\n",
       "      <th>...</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zune</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zuri</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zushi</th>\n",
       "      <th>zwart</th>\n",
       "      <th>zwijger</th>\n",
       "      <th>zwinky</th>\n",
       "      <th>zxcv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nb_times_appears</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  aaww  aba  aback  abagail  abandon  abandoned  abandoning  \\\n",
       "Nb_times_appears     1    2      1        1        4          7           1   \n",
       "\n",
       "                  abangan  abb  abba  ...   zumba  zune  zurg  zuri  zurich  \\\n",
       "Nb_times_appears        1    2     6  ...       3     6     1     1       2   \n",
       "\n",
       "                  zushi  zwart  zwijger  zwinky  zxcv  \n",
       "Nb_times_appears      1      1        1       1     1  \n",
       "\n",
       "[1 rows x 39073 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_by_terms = docs_terms.values > 0\n",
    "sum_by_terms = sum_by_terms.sum(axis=0)\n",
    "sum_by_terms = pd.DataFrame(data={'Nb_times_appears':sum_by_terms}, index=docs_terms.columns).T\n",
    "sum_by_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DF(t, t1=''):\n",
    "    \"\"\"\n",
    "    return either :\n",
    "        - the number of documents where t appears \n",
    "        - the number of documents where both t and t1 appear (if t1 specified)\n",
    "    \"\"\"\n",
    "    if t1 == '':\n",
    "        try:\n",
    "            return sum_by_terms.loc['Nb_times_appears', t]\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            return co_occurence.loc[t, t1]\n",
    "        except:\n",
    "            return 0\n",
    "DF('cat','dog') # 'cat' and 'dog' appear 5 times together in our tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, we will assume log(0) = 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logarithme(nombre):\n",
    "    return 0 if nombre==0 else np.log(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36769561060246758"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_size = len(x_train)\n",
    "def PMI(t,t1):\n",
    "    \"\"\"\n",
    "    return the Pointwise Mutual Information (PMI) between two terms\n",
    "    \"\"\"\n",
    "    if DF(t,t1) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return logarithme(DF(t,t1)) - logarithme(DF(t)) - logarithme(DF(t1)) + logarithme(D_size)\n",
    "PMI('kitty','happy') # test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI('kitty','happy') returns 0.368, while PMI('wedding','betrayal') would yield 0, as 'betrayal' doesn't occur in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic orientation of a word: SO(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO(\"adventure\") = 16.6390233941\n",
      "SO(\"wedding\") = 18.1816566727\n",
      "SO(\"darkness\") = 12.0822023553\n",
      "SO(\"honey\") = 19.2777473106\n",
      "SO(\"suicide\") = -5.5589319577\n",
      "SO(\"france\") = -9.45185374576\n"
     ]
    }
   ],
   "source": [
    "def SO(t):\n",
    "    pmi_plus, pmi_minus = 0, 0\n",
    "    for t1, t2 in zip(V_plus[:], V_minus[:]):\n",
    "        pmi_plus = pmi_plus + PMI(t, t1)\n",
    "        pmi_minus = pmi_minus + PMI(t, t2)\n",
    "    return pmi_plus - pmi_minus\n",
    "\n",
    "print('SO(\\\"adventure\\\") =', SO('adventure'))\n",
    "print('SO(\\\"wedding\\\") =', SO('wedding'))\n",
    "print('SO(\\\"darkness\\\") =', SO('darkness'))\n",
    "print('SO(\\\"honey\\\") =', SO('honey'))\n",
    "print('SO(\\\"suicide\\\") =', SO('suicide'))\n",
    "print('SO(\\\"france\\\") =', SO('france'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait...how comes France, the most visited country in the world ends up with such a bad semantic orientation ? \n",
    "\n",
    "<b>Answer :</b> a crash involving Air France happened, leading twitter users to express their condoleances...and therefore, using a negatively connoted vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodies air france crash ben found via\n",
      "confirmed missing air france flight crashed atlantica\n",
      "abt missing air france plane follow best air news source\n",
      "thinking air france plane passengers\n",
      "sad air france catastrophe\n",
      "france tragic\n",
      "prayers air france victims\n",
      "france sad\n",
      "miss tobs already fun france dont leave long pall love love love also love juju portsmouth\n",
      "parts airfrance found near coast senegal\n"
     ]
    }
   ],
   "source": [
    "for tweet in data.SentimentText.values[:6000]:\n",
    "    if 'france' in tweet:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic orientation of a tweet : SO(tweet)\n",
    "We assert that the semantic orientation of a tweet simply is the sum of its semantic orientations of its terms.\n",
    "$$SO\\text{(\"Python is fun !\")} = SO(\\text{\"Python\"}) + SO(\\text{\"is\"}) + SO(\\text{\"fun\"})$$\n",
    "So eventually, we are able to predict whether a tweet is positively connoted or not !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.039340540465389"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SO_tweet(tweet):\n",
    "    somme_SO = 0\n",
    "    for mot in tweet.split():\n",
    "        somme_SO = somme_SO + SO(mot)\n",
    "    return somme_SO\n",
    "\n",
    "SO_tweet(\"python is fun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Semantic Orientation approach, although intuitive, has several issues :\n",
    "- It does not capture :\n",
    "    - the negations, (e.g. \"not bad\") ; \n",
    "    - the intensity (e.g. : \"so succesful\")\n",
    "    - ironic sentences (e.g. : \"he played so weel football once again...\")\n",
    "- The prediction process is time-consuming.\n",
    "\n",
    "However, this approach has proved itself to be quiet efficient in finding how close a tweet could be to a positive (resp. negative) vocabulary, in a non-supervised design."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
